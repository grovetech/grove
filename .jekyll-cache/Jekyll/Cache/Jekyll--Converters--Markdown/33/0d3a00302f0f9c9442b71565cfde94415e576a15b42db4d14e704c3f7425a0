I"˙7<p>Your websites and your systems are only as good as the backup strategy that you have for them. While managing Mac‚Äôs you may think that having a strategy for website data backup is not relevant or important. Until you realize all of the technologies that you use on a daily basis that are actually moving to the web platform or cloud. For example OSX Wiki Server and Profile Manager 2 are all 100% web based technologies. If you use a web based ticketing system like Web Help Desk or Spiceworks again you need a strategy. Hitting more to home if you use Munki with MunkiWebAdmin or Casper then you need some kind of web based backup strategy.</p>

<p>Why should web based backups be treated separately or looked at with a different kind of strategy? Why cant I just use Time Machine or Crashplan? Well simply put you have many moving pieces, pieces that can be quickly backed up and recovered separately instead of performing lengthy system backups and restores. Do not get me wrong I am a huge proponent of system wide server backups but if MYSQL crashes or PHP has a corrupt configuration file a long restore may or may not get you back on track while a more targeted backup approach can have you back up and running in a shorter amount of time.</p>

<p>Mysql and Postgres for example can be dumped to a file on an hourly, daily or monthly basis and restored without impacting the other systems running on your server. In this article we will go over how to backup your MYSQL, Apache, PHP files into an offsite Amazon S3 bucket. Why S3? Its a cheap cost effective place to store backups as long as you cycle through them which is what I will be teaching you how to do.</p>

<p>In the end I came up with the a reasonably cute idea, and that is to keep between 28 and 31 backups: ie, all the data that I‚Äôm backup up is pushed into a folder named after the day-of-the-month in an S3 bucket, so today (7th June 2013) all my backups are going into a folder named something like s3://mybackups/07</p>

<p>In a month‚Äôs time (7th July 2013) this backup will be overwritten by the July 7th backup. That‚Äôs not a bad solution really. If you want longer backups you can hack the below script and have two scripts ‚Äì one for a one-backup-per-day strategy and also another copy of the script that stores by month name which essentially rotates by month. That would give you daily backups for the past month, and monthly backups for the past year. Useful.</p>

<p>For this walkthrough we will be using the command line tool s3cmd it allows you to connect to your Amazon S3 bucket and securely transmits your data to a folder in the bucket you specify. If you are not familiar with S3 I suggest you stop now and read up on Amazon S3 and see how it can work for you and your organization.</p>

<p>In terms of your standard webserver you want to backup your /var/www (or wherever you keep your htdocs), along with any config info, so I also backup /etc/apache2 /etc/php5 /etc/mysql /etc/cron.daily. Of course on top of that you‚Äôll need a backup of your database which you can get by calling mysqldump. Then compress the lot and chuck it up to s3. You‚Äôll want to use s3cmd for this.</p>

<p><strong>Step 1: Install Home Brew</strong><br />
I am a huge fan of HomeBrew its a great command line tool helper that allows you to install these awesome utilities on your Mac through your terminal. The easiest way to install various Unix tools and open source software onto OS X is via a package manager or repository, unfortunately OS X doesn‚Äôt come with one, but fortunately there are some good folks that care.They come in the form of <a href="https://mxcl.github.com/homebrew/">Homebrew</a>. Homebrew isn‚Äôt the only option, also available is MacPorts and Fink but Homebrew is the newest and easiest of the trio. Its fully Compatible in OSX 10.8 Mountain Lion.</p>

<p><strong>Get Xcode</strong><br />
Get Xcode from the Apple app store, free download version, then install it and launch it from the /Applications folder. Go to Xcode preferences and then look in the ‚ÄòDownloads‚Äô button. Install the command line tools from the preferences of Xcode.</p>

<p><strong>Install Homebrew</strong><br />
To download install Homebrew run the install script on the command line as below and let the script do its thing</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">ruby &lt;<span class="o">(</span>curl <span class="nt">-fsSkL</span> raw.github.com/mxcl/homebrew/go<span class="o">)</span></code></pre></figure>

<p>Download and install XQuartz brew will moan as it is no longer installed as part of 10.8 and Xcode. After installing and as suggested in the command line, to check for any issues with the install run.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">brew doctor</code></pre></figure>

<p>If upgrading from a previous OSX version, update Xcode location</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">sudo </span>xcode-select <span class="nt">-switch</span> /Applications/Xcode.app/Contents/Developer</code></pre></figure>

<p><strong>Step #2 Install s3cmd and gpg (needed for encrypted transfers.)</strong></p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">brew <span class="nb">install </span>s3cmd

brew <span class="nb">install </span>gpg</code></pre></figure>

<p><strong>Step #3 Configure s3cmd</strong></p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">s3cmd <span class="nt">--configure</span> </code></pre></figure>

<p>When prompted paste in your Access key, Secret key and encryption password, the encyption password has to be made by you. The path to gpg is</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">/usr/local/bin/gpg</code></pre></figure>

<p>This is important: Do NOT configure s3cmd with your root AWS credentials - yes it will work, but would you store your root server password in a plaintext file? No, and your AWS credentials give the holder access to unlimited resources, your billing details, your machine images, everything. <a href="https://www.youtube.com/watch?v=ySl1gdH_7bY" target="_blank">Just watch this 2-minute you-tube video on creating AWS users &amp; groups with restricted access, create a new user/group that only has access to S3 and use those credentials to configure s3</a>. It‚Äôs not hard, it‚Äôll take you just a few minutes to do. Then wait a couple more minutes for these new credentials to propagate through amazon‚Äôs systems and you‚Äôre ready to carry on.</p>

<p><strong>Step #4 Automate the backup</strong></p>

<p>Modify the following script to suit your purposes:</p>

<ol>
  <li>Specify the names of your mysql databases in that you need backing up in DATABASES</li>
  <li>Add mysql login details for each DB in the format: databasename_USER and databasename_PW</li>
  <li>Specify which directories to backup in DIRECTORIES - for me that is config stuff and my /var/www</li>
  <li>Specify the name of the s3 bucket you‚Äôre going to backup into in the S3_BUCKET_URL</li>
</ol>

<p>The script also assumes you have tar and gzip installed, but I‚Äôll assume you can figure that bit out for yourself.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">## Specify data base schemas to backup and credentials</span>

 <span class="nv">DATABASES</span><span class="o">=</span><span class="s2">"wp myotherdb"</span>

 

 <span class="c">## Syntax databasename as per above _USER and _PW</span>

 <span class="nv">wp_USER</span><span class="o">=</span>username

 <span class="nv">wp_PW</span><span class="o">=</span>password

 <span class="nv">myotherdb_USER</span><span class="o">=</span>username

 <span class="nv">myotherdb_PW</span><span class="o">=</span>password

 

 <span class="c">## Specify directories to backup (it's clever to use relaive paths)</span>

 <span class="nv">DIRECTORIES</span><span class="o">=</span><span class="s2">"/var/www root etc/cron.daily etc/cron.monthly etc/apache2 etc/mysql etc/php5"</span> 

 

 <span class="c">## Initialize some variables</span>

 <span class="nv">DATE</span><span class="o">=</span><span class="si">$(</span><span class="nb">date</span> +%d<span class="si">)</span>

 <span class="nv">BACKUP_DIRECTORY</span><span class="o">=</span>/tmp/backups

 <span class="nv">S3_CMD</span><span class="o">=</span><span class="s2">"s3cmd"</span>

 

 <span class="c">## Specify where the backups should be placed</span>

 <span class="nv">S3_BUCKET_URL</span><span class="o">=</span>s3://mybackupbucket/<span class="nv">$DATE</span>/

 

 <span class="c">## The script</span>

 <span class="nb">cd</span> /

 <span class="nb">mkdir</span> <span class="nt">-p</span> <span class="nv">$BACKUP_DIRECTORY</span>

 <span class="nb">rm</span> <span class="nt">-rf</span> <span class="nv">$BACKUP_DIRECTORY</span>/<span class="k">*</span>

 

 <span class="c">## Backup MySQL:s</span>

 <span class="k">for </span>DB <span class="k">in</span> <span class="nv">$DATABASES</span>

 <span class="k">do

 </span><span class="nv">BACKUP_FILE</span><span class="o">=</span><span class="nv">$BACKUP_DIRECTORY</span>/<span class="k">${</span><span class="nv">DB</span><span class="k">}</span>.sql

 <span class="nv">USER</span><span class="o">=</span><span class="si">$(</span><span class="nb">eval echo</span> <span class="se">\$</span><span class="k">${</span><span class="nv">DB</span><span class="k">}</span>_USER<span class="si">)</span>

 <span class="nv">PASSWORD</span><span class="o">=</span><span class="si">$(</span><span class="nb">eval echo</span> <span class="se">\$</span><span class="k">${</span><span class="nv">DB</span><span class="k">}</span>_PW<span class="si">)</span>

 /usr/bin/mysqldump <span class="nt">-v</span> <span class="nt">-u</span> <span class="nv">$USER</span> <span class="nt">--password</span><span class="o">=</span><span class="nv">$PASSWORD</span> <span class="nt">-h</span> localhost <span class="nt">-r</span> <span class="nv">$BACKUP_FILE</span> <span class="nv">$DB</span> 2&gt;&amp;#038<span class="p">;</span>1

 <span class="nb">gzip</span> <span class="nv">$BACKUP_FILE</span> 2&gt;&amp;#038<span class="p">;</span>1

 <span class="nv">$S3_CMD</span> put <span class="k">${</span><span class="nv">BACKUP_FILE</span><span class="k">}</span>.gz <span class="nv">$S3_BUCKET_URL</span> 2&gt;&amp;#038<span class="p">;</span>1

 <span class="k">done</span>

 

 <span class="c">## Backup of config directories</span>

 <span class="k">for </span>DIR <span class="k">in</span> <span class="nv">$DIRECTORIES</span>

 <span class="k">do

 </span><span class="nv">BACKUP_FILE</span><span class="o">=</span><span class="nv">$BACKUP_DIRECTORY</span>/<span class="si">$(</span><span class="nb">echo</span> <span class="nv">$DIR</span> | <span class="nb">sed</span> <span class="s1">'s/\//-/g'</span><span class="si">)</span>.tgz

 <span class="nb">tar </span>zcvf <span class="k">${</span><span class="nv">BACKUP_FILE</span><span class="k">}</span> <span class="nv">$DIR</span> 2&gt;&amp;#038<span class="p">;</span>1

 <span class="nv">$S3_CMD</span> put <span class="k">${</span><span class="nv">BACKUP_FILE</span><span class="k">}</span> <span class="nv">$S3_BUCKET_URL</span> 2&gt;&amp;#038<span class="p">;</span>1

 <span class="k">done</span></code></pre></figure>

<p>Then, assuming you‚Äôve called it something like backupToS3.sh, make it executable and test it</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">chmod</span> +x backupToS3.sh

<span class="nb">sudo</span> ./backupToS3.sh</code></pre></figure>

<p>Once you‚Äôve ironed out any issues simply copy it over to /etc/cron.daily so that it runs daily</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">sudo cp </span>backupToS3.sh /etc/cron.daily</code></pre></figure>

<p>Now, the above script does daily backups, but if you want to do monthly backups you simply need to make a copy of the file (since you‚Äôll likely want a daily and monthly backup rotation) and edit the DATE variable to use months rather than day-of-the-month. If you use the month number you‚Äôll probably want to either prefix the month number with the word ‚Äúmonth‚Äù, or pop them into a subdirectory called ‚Äúmonthly‚Äù, alternatively you could use the month name, for instance:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">DATE</span><span class="o">=</span><span class="si">$(</span><span class="nb">date</span> +%m<span class="si">)</span>        // month number

<span class="nv">DATE</span><span class="o">=</span><span class="si">$(</span><span class="nb">date</span> +%b<span class="si">)</span>        // 3-letter month name

<span class="nv">DATE</span><span class="o">=</span><span class="si">$(</span><span class="nb">date</span> +%B<span class="si">)</span>        // full month name

<span class="nv">DATE</span><span class="o">=</span><span class="si">$(</span><span class="nb">date</span> +%m-%B<span class="si">)</span>     // month number, dash, full month name</code></pre></figure>

<p>Then make it executable and test it as you did the previous script, and then copy it into cron.monthly</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">sudo cp </span>monthlyBackupToS3.sh /etc/cron.monthly</code></pre></figure>

<p>Presumably this will then fire on the first of the month (I haven‚Äôt checked), but you could always put it in cron.daily so that monthly backup is from the last day of its month (for previous months, the present month would be up to date).</p>

<p><strong>Why you should care about backups</strong><br />
You never want to be the one responsible for saving the day and you have no plan or ability to execute. After a file is deleted is a terrible time to come up with a backup strategy. Create one, write it down, document it, schedule it and then train people how to manage it in the event of an emergency. It really is something you need to take seriously and I strongly believe that the more planning you do today makes for a much less stressful tomorrow!</p>

:ET